{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artem/.conda/envs/teaching/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import gym.wrappers\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе мы реализуем алгоритм q-learning и научим модель эффективно играть в [breakout]('https://en.wikipedia.org/wiki/Breakout_(video_game)') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Укажем разрешение кадров игры https://gym.openai.com/envs/Breakout-v0/\n",
    "input_resolution = [210, 160, 3]\n",
    "# Разрешение после предобработки (см. ниже)\n",
    "result_resolution = [80, 80] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию, которая будет осуществлять предобработку извлеченных из игры кадров. В модуле [tf.image]('https://www.tensorflow.org/api_docs/python/tf/image') содержится множество функций для работы с изображениями, воспользуемся ими "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_state_processor():\n",
    "    # Выделим пространственную размерность изображения после предобработки\n",
    "    size = tf.constant(result_resolution[:2])\n",
    "    \n",
    "    with tf.variable_scope(\"state_processor\"):\n",
    "        input_state = tf.placeholder(shape=input_resolution, dtype=tf.uint8)\n",
    "        output = input_state\n",
    "        \n",
    "        # Обрежем рамку и интерфейс при помощи crop_to_bounding_box \n",
    "        output = tf.image.crop_to_bounding_box(output, 34, 0, 160, 160)\n",
    "        # Отмасштабируем изображение\n",
    "        output = tf.image.resize_images(output, size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        \n",
    "        # Если количество каналов не указанно, либо равно 1, то переводим иображение в grayscale\n",
    "        if len(result_resolution) <3 or result_resolution[2] == 1:\n",
    "            output = tf.image.rgb_to_grayscale(output)\n",
    "            output = tf.squeeze(output)\n",
    "\n",
    "    def process(sess, state):\n",
    "        return sess.run(output, { input_state: state })\n",
    "    \n",
    "    return process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключим [openai gym]('https://github.com/openai/gym'). Gym -- это специальная среда, для запуска симуляций, предназначенных для обучения алгоритмов  reinforcement learning. Так же нам понадобится [atari-py]('https://github.com/openai/atari-py'), содержащая в себе эмуляторы игр Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем игру из библиотеки \n",
    "env = gym.envs.make(\"Breakout-v0\")\n",
    "\n",
    "# Указываем директорию эксперимента. \n",
    "experiment_dir = os.path.abspath(\"~/Lesson5_practice/atari-experiments/{}\".format(env.spec.id))\n",
    "# Указываем директорию в которую будут сохраняться видеозаписи эпизодов, сыгранных алгоритмом\n",
    "monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "if not os.path.exists(monitor_path):\n",
    "    os.makedirs(monitor_path)\n",
    "\n",
    "# Частота записи видео\n",
    "record_video_every=2    \n",
    "\n",
    "# Monitor осуществляет запись видео на диск\n",
    "env = gym.wrappers.Monitor(env, monitor_path, \n",
    "                   video_callable=lambda count: count % record_video_every == 0 and count > 2,\n",
    "                      resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Пространство действий (https://gym.openai.com/envs/Breakout-v0/)\n",
    "actions = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_q_estimator(session, resolution, action_space_size=5, name = 'q_estimator'):  \n",
    "    with tf.variable_scope(name):\n",
    "        # Плейсхолдеры для кадров, оценок и действий\n",
    "        frame_pl = tf.placeholder(shape=[None] + resolution + [4], dtype=tf.float32, name=\"frame\")\n",
    "        reward_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"reward\")\n",
    "        action_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "       \n",
    "        # Реализация в соответствии со https://arxiv.org/pdf/1312.5602.pdf\n",
    "        conv_1 = tf.layers.conv2d(frame_pl, 32, 8, strides=(4,4), activation=tf.nn.relu, name=\"conv_1\")\n",
    "        conv_2 = tf.layers.conv2d(conv_1, 64, 4, strides=(2,2), activation=tf.nn.relu, name=\"conv_2\")\n",
    "        conv_3 = tf.layers.conv2d(conv_2, 128, 3, strides=(1,1), activation=tf.nn.relu, name=\"conv_3\")\n",
    "        flatten = tf.contrib.layers.flatten(conv_3)\n",
    "        dense1 = tf.layers.dense(flatten, 512, activation=tf.nn.relu, name=\"dense1\")\n",
    "        output = tf.layers.dense(dense1, action_space_size, name=\"output\")\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        mask = tf.one_hot(action_pl, action_space_size)\n",
    "        filtered_tensor = tf.multiply(output, mask)\n",
    "        action_prediction = tf.reduce_sum(filtered_tensor, axis=1)\n",
    "\n",
    "        temp = tf.squared_difference(reward_pl, action_prediction)\n",
    "        loss = tf.reduce_mean(temp)\n",
    "\n",
    "        train_op = tf.train.AdamOptimizer(0.00025).minimize(loss, global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "        writer = tf.summary.FileWriter('atari-experiments/Breakout-v0/tfboard', session.graph)\n",
    "        summary = tf.summary.merge(\n",
    "                [tf.summary.scalar(\"loss\", loss),\n",
    "                tf.summary.histogram(\"q_values\", output)])\n",
    "        \n",
    "\n",
    "    def predict(session, frame):\n",
    "        feed_dict = { \n",
    "            frame_pl: frame\n",
    "        }\n",
    "        return session.run(output, feed_dict)\n",
    "    \n",
    "    def update(session, frames, reward, action):\n",
    "        feed_dict = { \n",
    "            frame_pl: frames, \n",
    "            reward_pl: reward,\n",
    "            action_pl: action\n",
    "        }\n",
    "#         print (\"shape of action=\", action.shape)\n",
    "#         print (\"shape of action_pl\", action_pl.shape)\n",
    "#         print(\"action_pl=\", action_pl)\n",
    "        loss_value, _, smr, step = session.run([loss, train_op, summary, tf.train.get_or_create_global_step()], feed_dict)\n",
    "        \n",
    "#         print(loss_value)\n",
    "        \n",
    "        writer.add_summary(smr, step)\n",
    "        return loss_value\n",
    "    \n",
    "    return predict, update, writer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_policy(action_space_size, predict):\n",
    "    def eps_gready(session, frames, epsilon):\n",
    "        is_random = np.random.choice([True, False], p = [epsilon,1.0 - epsilon])\n",
    "        if is_random:\n",
    "            return np.random.choice(action_space_size)\n",
    "        q_vaues = predict(session, np.expand_dims(frames, 0))\n",
    "        return np.argmax(q_vaues)\n",
    "    return eps_gready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_to_play(session, \n",
    "                  num_episodes = 3000,\n",
    "                  memory_size = 500000, \n",
    "                  init_memory_size = 1000, \n",
    "                  discount_factor = 0.99,\n",
    "                  epsilon_start = 1.0, \n",
    "                  epsilon_end = 0.0, \n",
    "                  epsilon_decay_steps=100000,\n",
    "                  batch_size=128):\n",
    "    \n",
    "    \n",
    "    predict, update, writer = build_q_estimator(session, result_resolution, len(actions))\n",
    "    target_predict, _, __ = build_q_estimator(session, result_resolution, len(actions), name='target_estimator')\n",
    "    process = create_state_processor()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # The replay memory\n",
    "    memory = []\n",
    "\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    eps_gready_policy = make_policy(len(actions), predict)\n",
    "\n",
    "    global_step = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    for i_episode in range(1, num_episodes):\n",
    "        # Populate the replay memory with initial experience\n",
    "        print(\"Do not disturb. Playing games, i_episode={}\".format(i_episode))\n",
    "        \n",
    "        frame = env.reset()\n",
    "        processed_frame = process(session, frame)\n",
    "        current_frames = np.stack([processed_frame] * 4, axis=2)\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            episode_summary = tf.Summary()\n",
    "            epsi = epsilons[min(epsilon_decay_steps-1, max(global_step-init_memory_size, 0))]\n",
    "            episode_summary.value.add(simple_value=epsi, tag=\"epsilon\")\n",
    "            writer.add_summary(episode_summary, global_step)\n",
    "                            \n",
    "            action_index = eps_gready_policy(session, current_frames, epsi)\n",
    "            frame, reward, done, _ = env.step(actions[action_index])\n",
    "            processed_frame = process(session, frame)\n",
    "            next_frames = np.append(current_frames[:,:,1:], np.expand_dims(processed_frame, 2), axis=2)\n",
    "            \n",
    "            if len(memory) == memory_size:\n",
    "                memory.pop(0)\n",
    "                \n",
    "            memory.append((current_frames, \n",
    "                           reward, \n",
    "                           actions[action_index],  \n",
    "                           next_frames,\n",
    "                           done))\n",
    "            \n",
    "            global_step += 1\n",
    "            current_frames = next_frames\n",
    "\n",
    "            if done:        \n",
    "                break\n",
    "            \n",
    "            if global_step - init_memory_size < 0:\n",
    "                continue \n",
    "                \n",
    "            samples = random.sample(memory, batch_size)\n",
    "            \n",
    "            frames_batch, reward_batch, action_batch, next_frames_batch, done_batch  = map(np.array, zip(*samples))\n",
    "            \n",
    "            #print (\"shape of action_batch=\", action_batch.shape)\n",
    "            \n",
    "            q_values_next = target_predict(session, next_frames_batch)\n",
    "            q_values_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "            \n",
    "            if global_step % 1000 == 0:\n",
    "                print('copy model parameters')\n",
    "                copy_model_parameters(session, 'q_estimator', 'target_estimator')\n",
    "            \n",
    "            # Perform gradient descent update\n",
    "            #print (\"updating loss...\")\n",
    "            loss = update(session, frames_batch, q_values_batch, action_batch)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as session:\n",
    "    learn_to_play(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
